{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subject 1, class HandStart\n",
      "Train subject 1, class FirstDigitTouch\n",
      "Train subject 1, class BothStartLoadPhase\n",
      "Train subject 1, class LiftOff\n",
      "Train subject 1, class Replace\n",
      "Train subject 1, class BothReleased\n",
      "[0.7729809273519597, 0.7867231016041433, 0.7651435457751116, 0.7866273054629305, 0.8785932382700805, 0.8169049678300946]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "#############function to read data###########\n",
    "\n",
    "def prepare_data_train(fname):\n",
    "    \"\"\" read and prepare training data \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(fname)\n",
    "    # events file\n",
    "    events_fname = fname.replace('_data','_events')\n",
    "    # read event file\n",
    "    labels= pd.read_csv(events_fname)\n",
    "    clean=data.drop(['id' ], axis=1)#remove id\n",
    "    labels=labels.drop(['id' ], axis=1)#remove id\n",
    "    return  clean,labels\n",
    "\n",
    "def prepare_data_test(fname):\n",
    "    \"\"\" read and prepare test data \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(fname)\n",
    "    return data\n",
    "\n",
    "scaler= StandardScaler()\n",
    "def data_preprocess_train(X):\n",
    "    X_prep=scaler.fit_transform(X)\n",
    "    #do here your preprocessing\n",
    "    return X_prep\n",
    "def data_preprocess_test(X):\n",
    "    X_prep=scaler.transform(X)\n",
    "    #do here your preprocessing\n",
    "    return X_prep\n",
    "\n",
    "def fit(X,y):\n",
    "    # Do here you training\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "def predict(clf,X):\n",
    "    # do here your prediction\n",
    "    preds = clf.predict_proba(X)\n",
    "    return np.atleast_2d(preds[:,clf.classes_==1])\n",
    "\n",
    "##downsamplig naive like this is not correct, if you do not low pass filter.\n",
    "##this down sampling here it needed only to keep the script run below 10 minutes.\n",
    "## please do not downsample or use correct procedure to decimate data without alias\n",
    "subsample=100 # training subsample.if you want to downsample the training data\n",
    "#######columns name for labels#############\n",
    "cols = ['HandStart','FirstDigitTouch',\n",
    "        'BothStartLoadPhase','LiftOff',\n",
    "        'Replace','BothReleased']\n",
    "\n",
    "#######number of subjects###############\n",
    "subjects = range(1,2)\n",
    "ids_tot = []\n",
    "pred_tot = []\n",
    "y_tot = []\n",
    "auc_tot = []\n",
    "\n",
    "###loop on subjects and 8 series for train data + 2 series for test data\n",
    "for subject in subjects:\n",
    "    y_raw= []\n",
    "    raw = []\n",
    "    ################ READ DATA ################################################\n",
    "    fnames =  glob('../train/subj%d_series*_data.csv' % (subject))\n",
    "    for fname in fnames:\n",
    "        data,labels=prepare_data_train(fname)\n",
    "        raw.append(data)\n",
    "        y_raw.append(labels)\n",
    "\n",
    "    X = pd.concat(raw)\n",
    "    y = pd.concat(y_raw)\n",
    "    #transform in numpy array\n",
    "    #transform train data in numpy array\n",
    "    X_train =np.asarray(X.astype(float))\n",
    "    y = np.asarray(y.astype(float))\n",
    "\n",
    "\n",
    "    ################ Read test data #####################################\n",
    "    #\n",
    "    fnames =  glob('../test/subj%d_series*_data.csv' % (subject))\n",
    "    test = []\n",
    "    idx=[]\n",
    "    for fname in fnames:\n",
    "        data=prepare_data_test(fname)\n",
    "        test.append(data)\n",
    "        idx.append(np.array(data['id']))\n",
    "    X_test= pd.concat(test)\n",
    "    ids=np.concatenate(idx)\n",
    "    ids_tot.append(ids)\n",
    "    X_test=X_test.drop(['id' ], axis=1)#remove id\n",
    "    #transform test data in numpy array\n",
    "    X_test =np.asarray(X_test.astype(float))\n",
    "\n",
    "\n",
    "    ################ Train classifiers ########################################\n",
    "    lr = LogisticRegression()\n",
    "    pred = np.empty((X_test.shape[0],6))\n",
    "    \n",
    "    X_train=data_preprocess_train(X_train)\n",
    "    X_test=data_preprocess_test(X_test)\n",
    "    \n",
    "    for i in range(6):\n",
    "        y_train= y[:,i]\n",
    "        print('Train subject %d, class %s' % (subject, cols[i]))\n",
    "        lr.fit(X_train[::subsample,:],y_train[::subsample])\n",
    "        pred[:,i] = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "    pred_tot.append(pred)\n",
    "    y_tot.append(y)\n",
    "    auc = [roc_auc_score(y[:,i],pred[:,i]) for i in range(6)]     \n",
    "    auc_tot.append(auc)\n",
    "    print(auc)\n",
    "    \n",
    "pred_tot = np.concatenate(pred_tot)\n",
    "y_tot = np.concatenate(y_tot)\n",
    "global_auc = [roc_auc_score(y_tot[:,i],pred_tot[:,i]) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
