{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "#for spliting data into traning and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#for Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "#for random forest and boosting\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#for LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#for metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,f1_score\n",
    "\n",
    "#PCA for Dimension Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#to perform cross validation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: \n",
    "1. **change to `subjects = range(1, 13)`** to loop through all the subjects\n",
    "2. **In order to run:** Move series 7 and 8 for each subject into new directory called `test`. The `train` directory should contain series 1 through 6 for each subject. Also ensure file path is correct, currently it assumes the data is one directory back from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data csv\n",
    "def read_data(fname):\n",
    "    \"\"\" read and prepare training data \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(fname)\n",
    "    # events file\n",
    "    events_fname = fname.replace('_data','_events')\n",
    "    # read event file\n",
    "    labels= pd.read_csv(events_fname)\n",
    "    clean=data.drop(['id' ], axis=1)#remove id\n",
    "    labels=labels.drop(['id' ], axis=1)#remove id\n",
    "    return  clean,labels\n",
    "\n",
    "# standardise features in preprocessesing\n",
    "scaler= StandardScaler()\n",
    "def preprocess(X, t):\n",
    "    if (t == \"train\"):\n",
    "        X_prep = scaler.fit_transform(X)\n",
    "    else:\n",
    "         X_prep = scaler.transform(X)\n",
    "    return X_prep\n",
    "\n",
    "# train classifier\n",
    "def fit(X,y):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "# predict\n",
    "def predict(clf,X):\n",
    "    preds = clf.predict_proba(X)\n",
    "    return np.atleast_2d(preds[:,clf.classes_ == 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data and training and running classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subject 1, class HandStart\n",
      "Training subject 1, class FirstDigitTouch\n",
      "Training subject 1, class BothStartLoadPhase\n",
      "Training subject 1, class LiftOff\n",
      "Training subject 1, class Replace\n",
      "Training subject 1, class BothReleased\n",
      "\u001b[1m AUC of Subject  1 [0.7429219447931625, 0.7010134876469049, 0.7154205634718444, 0.778910967829658, 0.8570135564967338, 0.8113825798169321]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subsample = 100 # training subsample\n",
    "\n",
    "subjects = range(1,2) # total number of subjects \n",
    "pred_tot = []         # all the predictions\n",
    "label_tot = []        # all the labels\n",
    "auc_tot = []          # all the auc scors\n",
    "cols = ['HandStart', 'FirstDigitTouch', 'BothStartLoadPhase','LiftOff', 'Replace', 'BothReleased'] # hand movements\n",
    "\n",
    "# looping through the subjects to get train and test data for each subject's series (trials)\n",
    "# for each subject, train data is series 1 to 6, test data is series 7 and 8\n",
    "for subject in subjects:\n",
    "    \n",
    "    # get train data\n",
    "    train_files = glob('../train/subj%d_series*_data.csv' % (subject))\n",
    "    label_raw = []\n",
    "    data_raw = []\n",
    "\n",
    "    for f in train_files:\n",
    "        data, labels = read_data(f)\n",
    "        data_raw.append(data)\n",
    "        label_raw.append(labels)\n",
    "\n",
    "    X_train = np.array(pd.concat(data_raw))\n",
    "    y_train = np.array(pd.concat(label_raw))\n",
    "\n",
    "    # get test data\n",
    "    test_files =  glob('../test/subj%d_series*_data.csv' % (subject))\n",
    "    label_raw_test = []\n",
    "    data_raw_test = []\n",
    "    \n",
    "    for f in test_files:\n",
    "        data, labels = read_data(f)\n",
    "        data_raw_test.append(data)\n",
    "        label_raw_test.append(labels)\n",
    "\n",
    "    X_test = np.array(pd.concat(data_raw_test))\n",
    "    y_test = np.array(pd.concat(label_raw_test))\n",
    "    \n",
    "    # train classifiers\n",
    "    lr = LogisticRegression()\n",
    "    pred = np.empty((X_test.shape[0],6))\n",
    "    \n",
    "    X_train = preprocess(X_train, \"train\")\n",
    "    X_test = preprocess(X_test, \"test\")\n",
    "    \n",
    "    # train for each movement \n",
    "    for i in range(6):\n",
    "        y = y_train[:,i]\n",
    "        print('Training subject %d, class %s' % (subject, cols[i]))\n",
    "        lr.fit(X_train[::subsample,:], y[::subsample])\n",
    "        pred[:,i] = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # append all predictions and labels of the subjects\n",
    "    pred_tot.append(pred) \n",
    "    label_tot.append(y_train)\n",
    "    \n",
    "    # get auc score for each class for this subject\n",
    "    auc = [roc_auc_score(y_test[:,i],pred[:,i]) for i in range(6)] \n",
    "        \n",
    "    # append all auc scores of the subjects \n",
    "    auc_tot.append(auc)\n",
    "    \n",
    "    print(\"\\033[1m AUC of Subject \", subject, auc)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. make df or something to store all the AUC arrays for each class for each subject\n",
    "2. average over the subjects to get the average AUC score per class over all the subjects\n",
    "3. use other classifiers\n",
    "4. graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X,Y)\n",
    "X = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, Y_train, Y_test = train_test_split(X,Y, train_size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#Hyperparameters \n",
    "Penalty_list = [ 'l1', 'l2','elasticnet']\n",
    "C_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "param_grid = {'penalty': Penalty_list, 'C': C_list}\n",
    "model = LogisticRegression() \n",
    "grid_search = GridSearchCV(estimator=model,param_grid=param_grid,cv=10,scoring = 'f1')\n",
    "#this does cv and also train using best hyperparamater\n",
    "grid_search.fit(X_train,Y_train)\n",
    "\n",
    "#Find optimal hyper-paramater\n",
    "best_hyperparam = grid_search.best_params_\n",
    "    \n",
    "##trainng error and cv error\n",
    "cross_val_errors = (1- grid_search.cv_results_['mean_test_score'])\n",
    "train_error = sum(grid_search.best_estimator_.predict(X_train) != Y_train)/(len(Y_train))\n",
    "\n",
    "#test and optain testing accuracy\n",
    "test_error = sum(grid_search.best_estimator_.predict(X_test) != Y_test)/(len(Y_test))\n",
    "\n",
    "print(cross_val_errors)\n",
    "print(best_hyperparam)\n",
    "print(1-train_error)\n",
    "print(1-test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#Hyperparameters \n",
    "C_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "Kernel_list= ['linear']\n",
    "\n",
    "param_grid = {'C':C_list, 'kernel': Kernel_list}\n",
    "model = SVC() \n",
    "grid_search = GridSearchCV(estimator=model,param_grid=param_grid,cv=10,scoring = 'f1')\n",
    "#this does cv and also train using best hyperparamater\n",
    "grid_search.fit(X_train,Y_train)\n",
    "\n",
    "#Find optimal hyper-paramater\n",
    "best_hyperparam = grid_search.best_params_\n",
    "    \n",
    "##trainng error and cv error\n",
    "cross_val_errors = (1- grid_search.cv_results_['mean_test_score'])\n",
    "train_error = sum(grid_search.best_estimator_.predict(X_train) != Y_train)/(len(Y_train))\n",
    "\n",
    "#test and optain testing accuracy\n",
    "test_error = sum(grid_search.best_estimator_.predict(X_test) != Y_test)/(len(Y_test))\n",
    "\n",
    "print(cross_val_errors)\n",
    "print(best_hyperparam)\n",
    "print(1-train_error)\n",
    "print(1-test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#Hyperparamater\n",
    "numberOfTree_list = [5,10,50,100,500,1000,5000,10000]\n",
    "Criterion_list = ['gini', 'entropy']\n",
    "\n",
    "param_grid = {'n_estimators': numberOfTree_list, 'criterion': Criterion_list}\n",
    "model = RandomForestClassifier() \n",
    "grid_search = GridSearchCV(estimator=model,param_grid=param_grid,cv=10,scoring = 'f1')\n",
    "#this does cv and also train using best hyperparamater\n",
    "grid_search.fit(X_train,Y_train)\n",
    "    \n",
    "#Find optimal hyper-paramater\n",
    "best_hyperparam = grid_search.best_params_\n",
    "    \n",
    "##trainng error and cv error\n",
    "cross_val_errors = (1- grid_search.cv_results_['mean_test_score'])\n",
    "train_error = sum(grid_search.best_estimator_.predict(X_train) != Y_train)/(len(Y_train))\n",
    "\n",
    "#test and optain testing accuracy\n",
    "test_error = sum(grid_search.best_estimator_.predict(X_test) != Y_test)/(len(Y_test))\n",
    "\n",
    "print(cross_val_errors)\n",
    "print(best_hyperparam)\n",
    "print(1-train_error)\n",
    "print(1-test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#Hyperparamater\n",
    "n_estimators_list = [10,50,100,500]\n",
    "learningRate_list = [0.0001,0.001,0.01,0.1,1.0]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators_list, 'learning_rate': learningRate_list}\n",
    "model = AdaBoostClassifier() \n",
    "grid_search = GridSearchCV(estimator=model,param_grid=param_grid,cv=10,scoring = 'f1')\n",
    "#this does cv and also train using best hyperparamater\n",
    "grid_search.fit(X_train,Y_train)\n",
    "    \n",
    "#Find optimal hyper-paramater\n",
    "best_hyperparam = grid_search.best_params_\n",
    "    \n",
    "##trainng error and cv error\n",
    "cross_val_errors = (1- grid_search.cv_results_['mean_test_score'])\n",
    "train_error = sum(grid_search.best_estimator_.predict(X_train) != Y_train)/(len(Y_train))\n",
    "\n",
    "#test and optain testing accuracy\n",
    "test_error = sum(grid_search.best_estimator_.predict(X_test) != Y_test)/(len(Y_test))\n",
    "\n",
    "print(cross_val_errors)\n",
    "print(best_hyperparam)\n",
    "print(1-train_error)\n",
    "print(1-test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
