{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#for spliting data into traning and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#for Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "#for KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#for decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#for random forest and boosting\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#for LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#for metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,f1_score\n",
    "\n",
    "#PCA for Dimension Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#to perform cross validation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: \n",
    "1. **change to `subjects = range(1, 13)`** to loop through all the subjects\n",
    "2. **In order to run:** Move series 7 and 8 for each subject into new directory called `test`. The `train` directory should contain series 1 through 6 for each subject. Also ensure file path is correct, currently it assumes the data is one directory back from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data csv\n",
    "def read_data(fname):\n",
    "    \"\"\" read and prepare training data \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(fname)\n",
    "    # events file\n",
    "    events_fname = fname.replace('_data','_events')\n",
    "    # read event file\n",
    "    labels= pd.read_csv(events_fname)\n",
    "    clean=data.drop(['id' ], axis=1)#remove id\n",
    "    labels=labels.drop(['id' ], axis=1)#remove id\n",
    "    return  clean,labels\n",
    "\n",
    "# standardise features in preprocessesing\n",
    "scaler= StandardScaler()\n",
    "def preprocess(X, t):\n",
    "    if (t == \"train\"):\n",
    "        X_prep = scaler.fit_transform(X)\n",
    "    else:\n",
    "         X_prep = scaler.transform(X)\n",
    "    return X_prep\n",
    "\n",
    "# train classifier\n",
    "def myfit(X,y, model):\n",
    "  # clf = LogisticRegression()\n",
    "    model.fit(X,y)\n",
    "    return model\n",
    "\n",
    "# predict\n",
    "# def predict(clf,X):\n",
    "#     preds = clf.predict_proba(X)\n",
    "#     return np.atleast_2d(preds[:,clf.classes_ == 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for log Reg\n",
    "Penalty_list = [ 'l1', 'l2']\n",
    "C_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "logReg_param_grid = {'penalty': Penalty_list, 'C': C_list}\n",
    "\n",
    "#for Linear SVM\n",
    "C_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "Kernel_list= ['linear']\n",
    "svm_param_grid = {'C':C_list, 'kernel': Kernel_list, 'probability':True}\n",
    "\n",
    "#KNN \n",
    "K_list = [1, 3, 5, 7, 9]\n",
    "Algorithm_list = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "knn_param_grid = {'n_neighbors': K_list, 'algorithm': Algorithm_list}\n",
    "\n",
    "#Decision Tree \n",
    "Criterion_list = ['gini', 'entropy']\n",
    "D_list = [1,2,3,4,5]\n",
    "decisionTree_param_grid = {'max_depth': D_list, 'criterion': Criterion_list}\n",
    "\n",
    "#Random Forest \n",
    "numberOfTree_list = [5,10,50,100,500,1000,5000,10000]\n",
    "Criterion_list = ['gini', 'entropy']\n",
    "randomForest_param_grid = {'n_estimators': numberOfTree_list, 'criterion': Criterion_list}\n",
    "\n",
    "#AdaBoost\n",
    "n_estimators_list = [10,50,100,500]\n",
    "learningRate_list = [0.0001,0.001,0.01,0.1,1.0]\n",
    "adaBoost_param_grid = {'n_estimators': n_estimators_list, 'learning_rate': learningRate_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare models\n",
    "# models = [id name, classifier, hypermater of the classifier for Cross Validation]\n",
    "models = []\n",
    "#models.append(('Logistic Regression', LogisticRegression(),logReg_param_grid))\n",
    "#models.append(('Support Vector Machine', SVC(),svm_param_grid ))\n",
    "models.append(('K-nearest Neighbors', KNeighborsClassifier(),knn_param_grid ))\n",
    "#models.append(('Decision Tree', DecisionTreeClassifier(),decisionTree_param_grid))\n",
    "# models.append(('Random Forest', RandomForestClassifier(),randomForest_param_grid))\n",
    "# models.append(('AdaBoost', AdaBoostClassifier(),adaBoost_param_grid))\n",
    "#models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data and training and running classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest Neighbors\n",
      "Training subject 1, class HandStart\n",
      "Training subject 1, class FirstDigitTouch\n"
     ]
    }
   ],
   "source": [
    "subsample = 100 # training subsample\n",
    "\n",
    "subjects = range(1,2) # total number of subjects \n",
    "pred_tot = []         # all the predictions\n",
    "label_tot = []        # all the labels\n",
    "auc_tot = []          # all the auc scors\n",
    "cols = ['HandStart', 'FirstDigitTouch', 'BothStartLoadPhase','LiftOff', 'Replace', 'BothReleased'] # hand movements\n",
    "\n",
    "# looping through the subjects to get train and test data for each subject's series (trials)\n",
    "# for each subject, train data is series 1 to 6, test data is series 7 and 8\n",
    "for subject in subjects:\n",
    "    \n",
    "    # get train data\n",
    "    train_files = glob('../train/subj%d_series*_data.csv' % (subject))\n",
    "    label_raw = []\n",
    "    data_raw = []\n",
    "\n",
    "    for f in train_files:\n",
    "        data, labels = read_data(f)\n",
    "        data_raw.append(data)\n",
    "        label_raw.append(labels)\n",
    "\n",
    "    X_train = np.array(pd.concat(data_raw))\n",
    "    y_train = np.array(pd.concat(label_raw))\n",
    "\n",
    "    # get test data\n",
    "    test_files =  glob('../test/subj%d_series*_data.csv' % (subject))\n",
    "    label_raw_test = []\n",
    "    data_raw_test = []\n",
    "    \n",
    "    for f in test_files:\n",
    "        data, labels = read_data(f)\n",
    "        data_raw_test.append(data)\n",
    "        label_raw_test.append(labels)\n",
    "\n",
    "    X_test = np.array(pd.concat(data_raw_test))\n",
    "    y_test = np.array(pd.concat(label_raw_test))\n",
    "    \n",
    "    # train classifiers\n",
    "#     lr = LogisticRegression()\n",
    "#     pred = np.empty((X_test.shape[0],6))\n",
    "\n",
    "    \n",
    "    for name, model, hyperparam in models:\n",
    "        print(name)\n",
    "        #pred = np.empty((X_test.shape[0],6))\n",
    "        roc_prob = np.empty((X_test.shape[0],6))\n",
    "    \n",
    "        X_train = preprocess(X_train, \"train\")\n",
    "        X_test = preprocess(X_test, \"test\")\n",
    "    \n",
    "        # train for each movement \n",
    "        for i in range(6):\n",
    "            y = y_train[:,i]\n",
    "            print('Training subject %d, class %s' % (subject, cols[i]))\n",
    "            \n",
    "            # run grid search\n",
    "            grid_search = GridSearchCV(estimator=model,param_grid=hyperparam,cv=10,scoring = 'roc_auc')\n",
    "            #this does cv and also train using best hyperparamater\n",
    "            grid_search.fit(X_train[::subsample,:], y[::subsample])\n",
    "            roc_prob[:,i] = grid_search.predict_proba(X_test)[:,i]\n",
    "            \n",
    "            #myfit(X_train[::subsample,:], y[::subsample], model)\n",
    "            #model.fit(X_train[::subsample,:], y[::subsample])\n",
    "            #pred[:,i] = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        # append all predictions and labels of the subjects\n",
    "        #pred_tot.append(pred) \n",
    "        pred_tot.append(roc_prob)\n",
    "        label_tot.append(y_train)\n",
    "    \n",
    "        # get auc score for each class for this subject\n",
    "        #auc = [roc_auc_score(y_test[:,i],pred[:,i]) for i in range(6)] \n",
    "        auc = [roc_auc_score(y_test[:,i],roc_prob[:,i]) for i in range(6)] \n",
    "        \n",
    "        # append all auc scores of the subjects \n",
    "        auc_tot.append(auc)\n",
    "    \n",
    "        print(\"AUC of Subject \", subject, auc)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. make df or something to store all the AUC arrays for each class for each subject\n",
    "2. average over the subjects to get the average AUC score per class over all the subjects\n",
    "3. use other classifiers\n",
    "4. graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nOTE: I AM ASSUMING THAT DATA ARE IN X AND THE TARGETS ARE IN Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X,Y)\n",
    "X = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "def train_model(result,name,model,hyperparam):\n",
    "    trainingErrorForRepeats = []\n",
    "    testingErrorForRepeats = []\n",
    "   \n",
    "    #run 3 times and obtain mean and standard devation of testing/ testing error\n",
    "    for repeat in range(3):\n",
    "        #find optimal combination of hyperaramaters\n",
    "        grid_search = GridSearchCV(estimator=model,param_grid=hyperparam,cv=10,scoring = 'roc_auc')\n",
    "        #this does cv and also train using best hyperparamater\n",
    "        grid_search.fit(X_train,Y_train)\n",
    "        #Find optimal hyper-paramater\n",
    "        best_hyperparam = grid_search.best_params_\n",
    "    \n",
    "        ##trainng and testing error\n",
    "        train_error = grid_search.score(X_train,Y_train)\n",
    "        test_error = grid_search.score(X_test,Y_test)\n",
    "        trainingErrorForRepeats.append(train_error)\n",
    "        testingErrorForRepeats.append(test_error)\n",
    "        \n",
    "    trainMean = np.mean(trainingErrorForRepeats)\n",
    "    trainStd = np.std(trainingErrorForRepeats)\n",
    "    trainError = str(trainMean) + u\"\\u00B1\"+ str(trainStd)\n",
    "   \n",
    "    testMean = np.mean(testingErrorForRepeats)\n",
    "    testStd = np.std(testingErrorForRepeats)\n",
    "    testError = str(testMean) + u\"\\u00B1\"+ str(testStd)\n",
    "    \n",
    "    #toAdd = [name, best_hyperparam, train_error, test_error]\n",
    "    toAdd = [name, best_hyperparam, trainError, testError]\n",
    "    result.append(toAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL MEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split it into train and test\n",
    "train_size = [0.2,0.5,0.8]\n",
    "#results\n",
    "result = []\n",
    "for size in train_size:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, train_size=size)\n",
    "    for name, model, hyperparam in models:\n",
    "        train_model(result,name,model,hyperparam)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training error/ testing error visulization for 3 differnt train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip result into multiple based on what it refeers to\n",
    "\n",
    "modelNameResult = []\n",
    "bestHyperparamaterResult = []\n",
    "trainingErrorResult = []\n",
    "testingErrorResult = []\n",
    "\n",
    "for row in result:\n",
    "    modelNameResult.append(row[0])\n",
    "    bestHyperparamaterResult.append(row[1])\n",
    "    trainingErrorResult.append(row[2])\n",
    "    testingErrorResult.append(row[3])\n",
    "    \n",
    "\n",
    "\n",
    "tempDataframe = pd.DataFrame({'Classifier':modelNameResult,\n",
    "                          'Optimal Hyper-paramater':bestHyperparamaterResult,\n",
    "                          'Training F1 SCore':trainingErrorResult,\n",
    "                          'Testing F1 Score':testingErrorResult })\n",
    "\n",
    "train20Dataframe = tempDataframe[0:5]\n",
    "train50Dataframe = tempDataframe[6:11]\n",
    "train80Dataframe = tempDataframe[12:17]\n",
    "\n",
    "#pretty_print(tempDataframe)\n",
    "Print(\"training size: 20%\")\n",
    "pretty_print(train20Dataframe)\n",
    "Print(\"training size: 50%\")\n",
    "pretty_print(train50Dataframe)\n",
    "Print(\"training size: 80%\")\n",
    "pretty_print(train80Dataframe)\n",
    "\n",
    "pd.set_option('display.max_rows', 500000)\n",
    "pd.set_option('display.max_columns', 5000000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
